{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping rows or columns with 70% missing values\n",
    "threshold = 0.7\n",
    "#Dropping columns with missing value rate higher than threshold\n",
    "data = data[data.columns[data.isnull().mean() < threshold]]\n",
    "\n",
    "#Dropping rows with missing value rate higher than threshold\n",
    "data = data.loc[data.isnull().mean(axis=1) < threshold]\n",
    "\n",
    "\n",
    "##Numerical missing value handling\n",
    "#Filling all missing values with 0\n",
    "data = data.fillna(0)\n",
    "#Filling missing values with medians of the columns\n",
    "data = data.fillna(data.median())\n",
    "\n",
    "##Categorical missing value handling\n",
    "##imputing a category like “Other” might be more sensible, because in such a case, your imputation is likely to converge a random selection.\n",
    "#Max fill function for categorical columns\n",
    "data['column_name'].fillna(data['column_name'].value_counts()\n",
    ".idxmax(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the outlier rows with standard deviation\n",
    "factor = 3\n",
    "upper_lim = data['column'].mean () + data['column'].std () * factor\n",
    "lower_lim = data['column'].mean () - data['column'].std () * factor\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]\n",
    "\n",
    "#z-score can be used instead of the formula above. Z-score (or standard score) standardizes the distance between a value and the mean using the standard deviation.\n",
    "\n",
    "#Dropping the outlier rows with Percentiles\n",
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]\n",
    "\n",
    "#Capping the outlier rows with Percentiles\n",
    "#capping can affect the distribution of the data, thus it better not to exaggerate it.\n",
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "data.loc[(df[column] > upper_lim),column] = upper_lim\n",
    "data.loc[(df[column] < lower_lim),column] = lower_lim\n",
    "\n",
    "#Discover outliers with visualization tools \n",
    "# Box plot\n",
    "import seaborn as sns\n",
    "sns.boxplot(x=boston_df['DIS'])\n",
    "\n",
    "# Scatter Plots\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.scatter(boston_df['INDUS'], boston_df['TAX'])\n",
    "ax.set_xlabel('Proportion of non-retail business acres per town')\n",
    "ax.set_ylabel('Full-value property-tax rate per $10,000')\n",
    "plt.show()\n",
    "\n",
    "# z-score\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "z = np.abs(stats.zscore(boston_df))\n",
    "threshold = 3\n",
    "print(np.where(z > 3))\n",
    "\n",
    "# IQR\n",
    "Q1 = boston_df_o1.quantile(0.25)\n",
    "Q3 = boston_df_o1.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(boston_df_o1 < (Q1 - 1.5 * IQR)) |(boston_df_o1 > (Q3 + 1.5 * IQR))\n",
    "\n",
    "# For removing OT using z-score and IQR, above represents the calculations \n",
    "boston_df_o = boston_df_o[(z < 3).all(axis=1)]\n",
    "\n",
    "boston_df_out = boston_df_o1[~((boston_df_o1 < (Q1 - 1.5 * IQR)) |(boston_df_o1 > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "boston_df_out.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BINNING CATEGORICAL & NUMERICAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The main motivation of binning is to make the model more robust and prevent overfitting, however, it has a cost to the performance. \n",
    "- The trade-off between performance and overfitting is the key point of the binning process. \n",
    "-  for numerical columns, except for some obvious overfitting cases, binning might be redundant for some kind of algorithms, due to its effect on model performance.\n",
    "- for categorical columns, the labels with low frequencies probably affect the robustness of statistical models negatively. Thus, assigning a general category to these less frequent values helps to keep the robustness of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical Binning Example\n",
    "data['bin'] = pd.cut(data['value'], bins=[0,30,70,100], labels=[\"Low\", \"Mid\", \"High\"])\n",
    "\n",
    "#Categorical Binning Example\n",
    "conditions = [\n",
    "    data['Country'].str.contains('Spain'),\n",
    "    data['Country'].str.contains('Italy'),\n",
    "    data['Country'].str.contains('Chile'),\n",
    "    data['Country'].str.contains('Brazil')]\n",
    "\n",
    "choices = ['Europe', 'Europe', 'South America', 'South America']\n",
    "\n",
    "data['Continent'] = np.select(conditions, choices, default='Other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Transform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data you apply log transform must have only positive values, otherwise you receive an error. Also, you can add 1 to your data before transform it. Thus, you ensure the output of the transformation to be positive.\n",
    "- Log(x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Transform Example\n",
    "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "data['log+1'] = (data['value']+1).transform(np.log)\n",
    "#Negative Values Handling\n",
    "#Note that the values are different\n",
    "data['log'] = (data['value']-data['value'].min()+1) .transform(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONE HOT ENCODING AND LABEL ENCODER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Refer before project codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping Operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical Column Grouping\n",
    "#select the label with the highest frequency. In other words, this is the max operation for categorical columns,\n",
    "data.groupby('id').agg(lambda x: x.value_counts().index[0])\n",
    "\n",
    "#pivot table. This approach resembles the encoding method in the preceding step with a difference. \n",
    "#Instead of binary notation, it can be defined as aggregated functions for the values between grouped and encoded columns.\n",
    "#Pivot table Pandas Example\n",
    "data.pivot_table(index='column_to_group', columns='column_to_encode', values='aggregation_column', aggfunc=np.sum, fill_value = 0)\n",
    "\n",
    "\n",
    "#Numerical Column Grouping\n",
    "#Numerical columns are grouped using sum and mean functions in most of the cases. Both can be preferable according to the meaning of the feature.\n",
    "#sum_cols: List of columns to sum\n",
    "#mean_cols: List of columns to average\n",
    "grouped = data.groupby('column_to_group')\n",
    "\n",
    "sums = grouped[sum_cols].sum().add_suffix('_sum')\n",
    "avgs = grouped[mean_cols].mean().add_suffix('_avg')\n",
    "\n",
    "new_df = pd.concat([sums, avgs], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split function is a good option, however, there is no one way of splitting features. It depends on the characteristics of the column, how to split it. Let’s introduce it with two examples. First, a simple split function for an ordinary name column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.name\n",
    "#0  Luther N. Gonzalez\n",
    "#1    Charles M. Young\n",
    "#2        Terry Lawson\n",
    "#3       Kristen White\n",
    "#4      Thomas Logsdon\n",
    "\n",
    "#Extracting first names\n",
    "data.name.str.split(\" \").map(lambda x: x[0])\n",
    "#0     Luther\n",
    "#1    Charles\n",
    "#2      Terry\n",
    "#3    Kristen\n",
    "#4     Thomas\n",
    "\n",
    "#Extracting last names\n",
    "data.name.str.split(\" \").map(lambda x: x[-1])\n",
    "#0    Gonzalez\n",
    "#1       Young\n",
    "#2      Lawson\n",
    "#3       White\n",
    "#4     Logsdon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another case for split function is to extract a string part between two chars. The following example shows an implementation of this case by using two split functions in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#String extraction example\n",
    "data.title.head()\n",
    "#0                      Toy Story (1995)\n",
    "#1                        Jumanji (1995)\n",
    "#2               Grumpier Old Men (1995)\n",
    "#3              Waiting to Exhale (1995)\n",
    "#4    Father of the Bride Part II (1995)\n",
    "data.title.str.split(\"(\", n=1, expand=True)[1].str.split(\")\", n=1, expand=True)[0]\n",
    "#0    1995\n",
    "#1    1995\n",
    "#2    1995\n",
    "#3    1995\n",
    "#4    1995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCALING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalization (or min-max normalization) scale all values in a fixed range between 0 and 1. This transformation does not change the distribution of the feature and due to the decreased standard deviations, the effects of the outliers increases. Therefore, before normalization, it is recommended to handle the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "data['normalized'] = (data['value'] - data['value'].min()) / (data['value'].max() - data['value'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Standardization (or z-score normalization) scales the values while taking into account standard deviation. If the standard deviation of features is different, their range also would differ from each other. This reduces the effect of the outliers in the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['standardized'] = (data['value'] - data['value'].mean()) / data['value'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA AND TIME EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extracting the parts of the date into different columns: Year, month, day, etc.\n",
    "- Extracting the time period between the current date and columns in terms of years, months, days, etc.\n",
    "- Extracting some specific features from the date: Name of the weekday, Weekend or not, holiday or not, etc.\n",
    "- If you transform the date column into the extracted columns like above, the information of them become disclosed and machine learning algorithms can easily understand them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({'date':\n",
    "['01-01-2017',\n",
    "'04-12-2008',\n",
    "'23-06-1988',\n",
    "'25-08-1999',\n",
    "'20-02-1993',\n",
    "]})\n",
    "\n",
    "#Transform string to date\n",
    "data['date'] = pd.to_datetime(data.date, format=\"%d-%m-%Y\")\n",
    "\n",
    "#Extracting Year\n",
    "data['year'] = data['date'].dt.year\n",
    "\n",
    "#Extracting Month\n",
    "data['month'] = data['date'].dt.month\n",
    "\n",
    "#Extracting passed years since the date\n",
    "data['passed_years'] = date.today().year - data['date'].dt.year\n",
    "\n",
    "#Extracting passed months since the date\n",
    "data['passed_months'] = (date.today().year - data['date'].dt.year) * 12 + date.today().month - data['date'].dt.month\n",
    "\n",
    "#Extracting the weekday name of the date\n",
    "data['day_name'] = data['date'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>passed_years</th>\n",
       "      <th>passed_months</th>\n",
       "      <th>day_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2008-12-04</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>137</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1988-06-23</td>\n",
       "      <td>1988</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>383</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1999-08-25</td>\n",
       "      <td>1999</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>249</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1993-02-20</td>\n",
       "      <td>1993</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>327</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  year  month  passed_years  passed_months   day_name\n",
       "0 2017-01-01  2017      1             3             40     Sunday\n",
       "1 2008-12-04  2008     12            12            137   Thursday\n",
       "2 1988-06-23  1988      6            32            383   Thursday\n",
       "3 1999-08-25  1999      8            21            249  Wednesday\n",
       "4 1993-02-20  1993      2            27            327   Saturday"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Removal Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Outlier Removal Clustering ( ORC ) is a improved version of KMean with outlier removal in each iteration. As we all know that KMean is more sensitive with outliers, and might result into local optimal centroids. For unsupervised clustering KMean is the mainly used algorithm because which is very effective as well as easy to implement. For data which has lot of outliers still works well with KMean if we add outlier removal mechanism in each iteration of the KMean clustering. ORC is the name of the algorithm. This will ensure the centroid calculation won't be skewed by the points far away from the cluster centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports required to understand the dataset, get initial\n",
    "# intuition of how the data looks like.\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the data see its basic characteristics.\n",
    "df = pd.read_csv(\"./2d-cluster.csv\", index_col=0);\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see the data distribution in terms of historgram\n",
    "df1 = pd.DataFrame({'x': df.x, 'y': df.y}, columns=['x', 'y'])\n",
    "plt.figure()\n",
    "df1.plot.hist(alpha=0.3, stacked=True, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot it on 2D plane\n",
    "# df = pd.read_csv(\"data_2d.csv\", )\n",
    "from pandas.tools.plotting import scatter_plot\n",
    "plt.scatter(df.x, df.y, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets cluter the points with KMean\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# df = pd.read_csv(\"data_2d.csv\")\n",
    "# Fit the data into KMean model, default params are\n",
    "# KMeans(copy_x=True, init='k-means++', max_iter=300,\n",
    "#        n_clusters=3, n_init=10, n_jobs=1,\n",
    "#        precompute_distances='auto',\n",
    "#        random_state=None, tol=0.0001, verbose=0)\n",
    "model = KMeans(n_clusters=3, max_iter=300)\n",
    "model.fit(df)\n",
    "\n",
    "# Plot the First iteration of the kmean.\n",
    "colormap = np.array(['red', 'lime', 'blue'])\n",
    "plt.scatter(df.x, df.y, c=colormap[model.labels_], s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMean with Outlier removal (ORC)\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# Cluste size.\n",
    "K = 3\n",
    "MAX_ITER = 100;\n",
    "\n",
    "# List of cluster with its points in it.\n",
    "CLUSTER = defaultdict(list);\n",
    "\n",
    "# Kmean model.\n",
    "model = KMeans(n_clusters=K, max_iter=MAX_ITER)\n",
    "\n",
    "# Anomoly threshold. Need to be tuned to avoid over / under fitting.\n",
    "# T = 0.921\n",
    "T = 0.95\n",
    "\n",
    "# Data frames loaded from csv.\n",
    "df = pd.read_csv(\"./2d-cluster.csv\", index_col=0)\n",
    "\n",
    "\n",
    "def distance(x, y):\n",
    "    \"\"\"\n",
    "    Find distance between two points in a plain.\n",
    "    @param x: 2D point.\n",
    "    @param y: 2D point.\n",
    "    \n",
    "    @return euclidean distance between this point.\n",
    "    \"\"\"\n",
    "    d1 = x[0] - y[0];\n",
    "    d2 = x[1] - y[1];\n",
    "    distance = math.sqrt(d1*d1 + d2*d2)\n",
    "    return distance\n",
    "\n",
    "\n",
    "def print_cluster_details(clusters, centroids):\n",
    "    for index, cluster in clusters.iteritems():\n",
    "        print \"Cluster: {} size: {}\".format(index, len(cluster)) \n",
    "\n",
    "        \n",
    "def dump_cluster_points(df, labels):\n",
    "    \"\"\"\n",
    "    @param clusters: dataframe\n",
    "    \n",
    "    Dump ponts of the cluster in csv file named as cluster_{#index}.csv\n",
    "    \"\"\"\n",
    "    clusters = aggregate_cluster_points(df, labels)\n",
    "    for index, cluster in clusters.iteritems():\n",
    "        with open(\"cluster_{}.csv\".format(index), \"w\") as f:\n",
    "            f.write(\"\\n\".join([\"{},{}\".format(p[0], p[1]) for p in cluster]))\n",
    "            \n",
    "def aggregate_cluster_points(df, labels):\n",
    "    \"\"\"\n",
    "    Helper methods to aggregate the cluster points based on the label index.\n",
    "    \n",
    "    @param df: List of points or datapoints\n",
    "    @param labels: Cluster index list for each element in points.\n",
    "\n",
    "    @retrun List of cluster points, indexed with cluster index.\n",
    "    \"\"\"\n",
    "    clusters = defaultdict(list)\n",
    "    \n",
    "    for index, value in enumerate(labels):\n",
    "        clusters[value].append(df.values[index])\n",
    "        \n",
    "    return clusters\n",
    "    \n",
    "\n",
    "def get_outliers_and_strip_cluster(cluster_points, centroid):\n",
    "    \"\"\"\n",
    "    Apply ODIN algorithm to identify anomalies in the cluster and\n",
    "    strip it.\n",
    "    \n",
    "    Anomaly detection rule:- \n",
    "    \n",
    "    sqrt(point^2 - centroid^2) / max(points) > T === True then it's an anomaly.\n",
    "    \n",
    "    @param cluster_points: List of points in this cluster.\n",
    "    @param centroid: centroid of the cluster.\n",
    "    @return: outliers, new_cluster\n",
    "    \"\"\"\n",
    "    d_vector = np.array([distance(point, centroid)\n",
    "                         for point in cluster_points])\n",
    "    d_max = d_vector.max();\n",
    "    data = pd.DataFrame([distance(centroid, point) / d_max\n",
    "                         for point in cluster_points])\n",
    "#     print data.min(), d_max\n",
    "    outliers = filter(lambda row: distance(centroid, row) / d_max > T,\n",
    "                      cluster_points)\n",
    "    new_cluster = filter(lambda row: distance(centroid, row) / d_max <= T,\n",
    "                         cluster_points)\n",
    "#     print \"Outlier size\", outliers.shape\n",
    "#     print \"New Cluster size: \", new_cluster.shape\n",
    "#     print \"Original cluster size: \", len(cluster_points)\n",
    "    \n",
    "    return outliers, new_cluster\n",
    "\n",
    "\n",
    "def run_outlier_removal_clustering(df, max_iteration):\n",
    "    \"\"\"\n",
    "    Run ORC Outlier removal clustering on the datapoints.\n",
    "    \n",
    "    Clustering Algorithm - KMean\n",
    "    Outlier removal Algorithm - ODIN a Knn based outlier detection.\n",
    "    \"\"\"\n",
    "    orc_model = KMeans(n_clusters=K, max_iter=MAX_ITER)\n",
    "    OUTLIERS = []\n",
    "    for iteration in range(max_iteration):\n",
    "        # Iteration.\n",
    "        #print \"\\n\\n[{}] ===> Data before clustering: {}, Anomaly: {}\".format(\n",
    "        #iteration, df.shape, len(OUTLIERS))\n",
    "        orc_model.fit(df)\n",
    "        labels = orc_model.labels_\n",
    "\n",
    "        CLUSTER = aggregate_cluster_points(df, labels)\n",
    "        centroids = orc_model.cluster_centers_\n",
    "\n",
    "        NEW_CLUSTER = []\n",
    "        for index, cluster in CLUSTER.iteritems():\n",
    "            #print \"Cluster: {} size: {}\".format(index, len(cluster))\n",
    "            outlier, new_cluster = get_outliers_and_strip_cluster(cluster,\n",
    "                                                                  centroids[index])\n",
    "\n",
    "            OUTLIERS.extend(outlier)\n",
    "            NEW_CLUSTER.extend(new_cluster)\n",
    "\n",
    "        # Update the cluster with new cluster.\n",
    "        df = pd.DataFrame(data=NEW_CLUSTER)\n",
    "        \n",
    "    # Fit for the one more time, as the when loop exists we removed few anomolies.\n",
    "    orc_model.fit(df)\n",
    "\n",
    "    return df, orc_model, OUTLIERS\n",
    "    \n",
    "    \n",
    "# Run Clustering with Outlier removal algorithm.\n",
    "df, orc_model, outliers = run_outlier_removal_clustering(df, 5)\n",
    "\n",
    "\n",
    "\n",
    "# Dump the final cluster and anomalies into csv file.\n",
    "print_cluster_details(aggregate_cluster_points(df, orc_model.labels_),\n",
    "                      orc_model.cluster_centers_)\n",
    "print \"Total anomalies: {}\".format(len(outliers))\n",
    "print \"Exported the cluster and anomalies into csv files\"\n",
    "dump_cluster_points(df, orc_model.labels_)\n",
    "with open(\"anomalies.csv\", 'w') as f:\n",
    "    f.write(\"\\n\".join([\"{},{}\".format(p[0], p[1]) for p in outliers]))\n",
    "    \n",
    "\n",
    "# Plot the Original and new cluster after anomaly removal.\n",
    "plt.figure(figsize=(12,4))\n",
    "colormap = np.array(['red', 'lime', 'blue', 'green', 'yellow'])\n",
    "df.columns = ['x', 'y']\n",
    "\n",
    "data = pd.read_csv(\"./2d-cluster.csv\", index_col=0)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(data.x, data.y, s=20)\n",
    "plt.title(\"Without clustering\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "_kmean = model.fit(data)\n",
    "plt.scatter(data.x, data.y, c=colormap[_kmean.labels_], s=20)\n",
    "plt.title(\"KMean Clustering\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(df.x, df.y, c=colormap[orc_model.labels_], s=20)\n",
    "plt.title(\"ORC clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How I generated this sample cluster data.\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=1000,\n",
    "                  n_features=2,\n",
    "                  centers=3,\n",
    "                  center_box=(-5, 5)\n",
    "                  )\n",
    "plt.scatter(X[:, 0], X[:, 1], marker='o', c=y)\n",
    "\n",
    "# How to save the data smaples into csv file.\n",
    "\n",
    "_d = pd.DataFrame(X)\n",
    "_d.columns = ['x', 'y']\n",
    "_d.to_csv(\"./2d-cluster_new.csv\")\n",
    "#_d = pd.read_csv(\"./2d-cluster.csv\", index_col=0)\n",
    "\n",
    "#REFERENCES:\n",
    "#https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba\n",
    "#https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b\n",
    "#https://github.com/SharmaNatasha\n",
    "# https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/notebooks/Ch04_Feature_Engineering_and_Selection\n",
    "#https://haridas.in/outlier-removal-clustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
